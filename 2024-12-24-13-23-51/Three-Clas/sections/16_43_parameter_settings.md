
- vocab dim = 100: This parameter sets the dimensionality of the word vectors, mapping each word to a fixed-length vector. Smaller dimensions (e.g., 50â€“100) result in faster training and lower memory usage but capture limited semantic information. Larger dimensions (e.g., 300) can capture richer semantic information but require longer training times. This is commonly used in word embedding models like Word2Vec and GloVe.
- n iterations = 10: This specifies the number of iterations for model training. More iterations can improve the quality of word vectors but also increase computational time.
- n exposures = 10: This sets the word frequency threshold, retaining only words that appear more than 10 times. Filtering low-frequency words reduces noise. This parameter typically corresponds to the min count parameter in Word2Vec.
- window size = 7: This controls the size of the context window, considering up to 7 words to the left and right of a target word as context. A smaller window captures local relationships, while a larger window captures broader contexts.
- n epoch = 4: This sets the number of epochs for model training. Each epoch represents one complete pass through the entire dataset. Increasing the number of epochs can enhance model performance but may introduce overfitting risks.
- input length = 100 and maxlen = 100: These parameters ensure that input sequences are fixed at length 100. Shorter texts are padded with zeros to reach the desired length, while longer texts are truncated to fit. This maintains consistency in input data, facilitating efficient computation for models like LSTM and CNN. These parameters are commonly used in Keras's Embedding layer and the pad sequences function.
