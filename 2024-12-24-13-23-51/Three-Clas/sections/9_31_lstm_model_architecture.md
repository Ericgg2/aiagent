
Long Short-Term Memory (LSTM) networks are a type of Recurrent Neural Network (RNN) wellsuited for processing sequential data, capable of capturing long-term dependencies and sequence information within text data. Widely applied in natural language processing tasks, this paper opts to use an LSTM model independently for feature extraction and sentiment analysis on textual data, effectively modeling the sequential information of texts. The LSTM model primarily consists of an input layer, LSTM layer, and output layer, as illustrated in Figure 1.

The processed text sequences—having undergone tokenization and vectorization—are fed into the LSTM network. In the input layer, text data is transformed into fixed-length word vector sequences, serving as inputs to the LSTM. Through its unique gating mechanisms—the input gate, forget gate, and output gate—the LSTM network processes sequential data, effectively capturing significant contextual and temporal information within texts while avoiding the vanishing or exploding gradient problems common in traditional RNNs.

The output from the LSTM layer contains high-dimensional sequence features, which are then passed through a fully connected layer to further extract deep semantic information from the text. Finally, a Softmax classifier maps these features to a probability distribution across categories, completing the classification prediction for the sentiment analysis task. This overall model architecture is straightforward yet powerful, leveraging the advantages of LSTMs in sequence modeling to effectively mine emotional features from text data.
