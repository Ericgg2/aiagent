[1] Ekin Aky¨ urek, Mehul Damani, Linlu Qiu, Han Guo, Yoon Kim, and Jacob
Andreas. The surprising effectiveness of test-time training for abstract
reasoning, 2024.
[2] Anthropic. Claude 3.5 Sonnet. Anthropic blog , 2024. https://www.
anthropic.com/news/claude-3-5-sonnet (last accessed 26/11/2024).
[3] Pietro Baroni, Dov Gabbay, Massimilino Giacomin, and Leendert Van der
Torre, editors. Handbook of formal argumentation . College Publications,
2018.
20[4] Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc V
Le, Christopher R´ e, and Azalia Mirhoseini. Large language mon-
keys: Scaling inference compute with repeated sampling. arXiv preprint
arXiv:2407.21787 , 2024.
[5] Federico Castagna, Alexandra Garton, Peter McBurney, Simon Parsons,
Isabel Sassoon, and Elizabeth I. Sklar. EQRbot: A chatbot delivering
EQR argument-based explanations. Frontiers in Artificial Intelligence , 6,
2023.
[6] Federico Castagna, Nadin K¨ okciyan, Isabel Sassoon, Simon Parsons, and
Elizabeth Sklar. Computational argumentation-based chatbots: a survey.
Journal of Artificial Intelligence Research , 80:1271–1310, 2024.
[7] Federico Castagna, Simon Parsons, Isabel Sassoon, and Elizabeth I. Sklar.
Providing explanations via the EQR argument scheme. In Computational
Models of Argument: Proceedings of COMMA 2022 , 2022.
[8] Federico Castagna, Isabel Sassoon, and Simon Parsons. Can formal ar-
gumentative reasoning enhance LLMs performances? arXiv preprint
arXiv:2405.13036 , 2024.
[9] Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Kaijie Zhu, Hao Chen,
Linyi Yang, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, Wei Ye, Yue
Zhang, Yi Chang, Philip S. Yu, Qiang Yang, and Xing Xie. A survey
on evaluation of large language models. arXiv preprint arXiv:2307.03109 ,
2023.
[10] Yanxi Chen, Xuchen Pan, Yaliang Li, Bolin Ding, and Jingren Zhou. A
simple and provable scaling law for the test-time compute of large language
models. arXiv preprint arXiv:2411.19477 , 2024.
[11] Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas An-
gelopoulos, Tianle Li, Dacheng Li, Hao Zhang, Banghua Zhu, Michael
Jordan, Joseph E Gonzalez, et al. Chatbot arena: An open platform for
evaluating llms by human preference. arXiv preprint arXiv:2403.04132 ,
2024.
[12] Francois Chollet, Mike Knoop, Bryan Landers, Greg Kamradt, Hansueli
Jud, Walter Reade, and Addison Howard. Arc prize 2024. https:
//kaggle.com/competitions/arc-prize-2024 , 2024. Kaggle.
[13] Fran¸ cois Chollet. On the measure of intelligence, 2019.
[14] Roi Cohen, May Hamri, Mor Geva, and Amir Globerson. Lm vs lm: Detect-
ing factual errors via cross examination. arXiv preprint arXiv:2305.13281 ,
2023.
21[15] Phan Minh Dung. On the acceptability of arguments and its fundamental
role in nonmonotonic reasoning, logic programming and n-person games.
Artificial intelligence , 77(2):321–357, 1995.
[16] Gabriel Freedman, Adam Dejl, Deniz Gorur, Xiang Yin, Antonio Rago,
and Francesca Toni. Argumentative large language models for explainable
and contestable decision-making. arXiv preprint arXiv:2405.02079 , 2024.
[17] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with
selective state spaces. arXiv preprint arXiv:2312.00752 , 2023.
[18] Ankita Gupta, Ethan Zuckerman, and Brendan O’Connor. Harnessing toul-
min’s theory for zero-shot argument explication. In Proceedings of the 62nd
Annual Meeting of the Association for Computational Linguistics (Volume
1: Long Papers) , pages 10259–10276, 2024.
[19] Simeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting Qi, Martin Riddell,
Wenfei Zhou, James Coady, David Peng, Yujie Qiao, Luke Benson, et al.
Folio: Natural language reasoning with first-order logic. arXiv preprint
arXiv:2209.00840 , 2022.
[20] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika,
Dawn Song, and Jacob Steinhardt. Measuring massive multitask language
understanding. arXiv preprint arXiv:2009.03300 , 2020.
[21] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven
Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring
mathematical problem solving with the math dataset. arXiv preprint
arXiv:2103.03874 , 2021.
[22] Krystal Hu and Anna Tong. OpenAI and others seek new path
to smarter AI as current methods hit limitations. Reuters , 2024.
https://www.reuters.com/technology/artificial-intelligence/
openai-rivals-seek-new-path-smarter-ai-current-methods-hit-limitations-2024-11-11/
(last accessed 12/11/2024).
[23] Daniel Kahneman. Thinking, fast and slow. Farrar, Straus and Giroux ,
2011.
[24] Bingyi Kang, Yang Yue, Rui Lu, Zhijie Lin, Yang Zhao, Kaixin Wang, Gao
Huang, and Jiashi Feng. How far is video generation from world model: A
physical law perspective, 2024.
[25] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and
Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances
in neural information processing systems , 35:22199–22213, 2022.
[26] Opher Lieber, Barak Lenz, Hofit Bata, Gal Cohen, Jhonathan Osin, Itay
Dalmedigos, Erez Safahi, Shaked Meirom, Yonatan Belinkov, Shai Shalev-
Shwartz, et al. Jamba: A hybrid transformer-mamba language model.
arXiv preprint arXiv:2403.19887 , 2024.
22[27] F. Lin. An argument-based approach to non-monotonic reasoning. Com-
putational Intelligence , 9:254–267, 1993.
[28] F. Lin and Y. Shoham. Argument systems: a uniform basis for nonmono-
tonic reasoning. In Proceedings of the 1st International Conference on
Knowledge Representation and Reasoning , pages 245–255, San Mateo, CA,
1989. Morgan Kaufmann.
[29] R. P. Loui. Defeat among arguments: a system of defeasible inference.
Computational Intelligence , 3(3):100–106, 1987.
[30] Hugo Mercier and Dan Sperber. Why do humans reason? Arguments for
an argumentative theory. 34(2):57–74, 2011.
[31] Rachel Metz, Shirin Ghaffary, Dina Bass, and Julia Love. OpenAI, Google
and Anthropic are struggling to build more advanced AI. Bloomberg ,
2024. https://www.bloomberg.com/news/articles/2024-11-13/
openai-google-and-anthropic-are-struggling-to-build-more-advanced-ai
(last accessed 20/11/2024).
[32] Iman Mirzadeh, Keivan Alizadeh, Hooman Shahrokhi, Oncel Tuzel, Samy
Bengio, and Mehrdad Farajtabar. GSM-Symbolic: Understanding the limi-
tations of mathematical reasoning in large language models. arXiv preprint
arXiv:2410.05229 , 2024.
[33] R. C. Moore. Semantical considerations on nonmonotonic logic. Artificial
Intelligence , 25:75–94, 1985.
[34] Yaniv Nikankin, Anja Reusch, Aaron Mueller, and Yonatan Belinkov.
Arithmetic without algorithms: Language models solve math with a bag
of heuristics. arXiv preprint arXiv:2410.21272 , 2024.
[35] OpenAI. Introducing ChatGPT. OpenAI blog , 2022. https://openai.
com/index/chatgpt/ (last accessed 26/11/2024).
[36] OpenAI. Gpt-4o system card. arXiv preprint arXiv:2410.21276 , 2024.
[37] OpenAI. Learning to reason with LLMs. OpenAI blog , 2024. https:
//openai.com/index/learning-to-reason-with-llms/ (last accessed
6/12/2024).
[38] Stephanie Palazzolo, Erin Woo, and Amir Efrati. OpenAI Shifts
Strategy as Rate of ‘GPT’ AI Improvements Slows. The In-
formation , 2024. https://www.theinformation.com/articles/
openai-shifts-strategy-as-rate-of-gpt-ai-improvements-slows
(last accessed 12/11/2024).
[39] J. L. Pollock. Defeasible reasoning. Cognitive Science , 11:481–518, 1987.
[40] J. L. Pollock. How to reason defeasibly. Artificial Intelligence , 57:1–42,
1992.
23[41] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty,
Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R Bow-
man. Gpqa: A graduate-level google-proof q&a benchmark. arXiv preprint
arXiv:2311.12022 , 2023.
[42] R. Reiter. A logic for default reasoning. Artificial Intelligence , 13:81–132,
1980.
[43] Pranab Sahoo, Ayush Kumar Singh, Sriparna Saha, Vinija Jain, Samrat
Mondal, and Aman Chadha. A systematic survey of prompt engineering
in large language models: Techniques and applications. arXiv preprint
arXiv:2402.07927 , 2024.
[44] Robin Smith. Aristotle’s Logic. In Edward N. Zalta and Uri Nodelman,
editors, The Stanford Encyclopedia of Philosophy . Metaphysics Research
Lab, Stanford University, Winter 2022 edition, 2022.
[45] Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm
test-time compute optimally can be more effective than scaling model pa-
rameters. arXiv preprint arXiv:2408.03314 , 2024.
[46] Saurabh Srivastava, Anto PV, Shashank Menon, Ajay Sukumar, Alan Phili-
pose, Stevin Prince, Sooraj Thomas, et al. Functional benchmarks for ro-
bust evaluation of reasoning performance, and the reasoning gap. arXiv
preprint arXiv:2402.19450 , 2024.
[47] Mirac Suzgun, Nathan Scales, Nathanael Sch¨ arli, Sebastian Gehrmann,
Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi,
Denny Zhou, , and Jason Wei. Challenging big-bench tasks and whether
chain-of-thought can solve them. arXiv preprint arXiv:2210.09261 , 2022.
[48] Sijun Tan, Siyuan Zhuang, Kyle Montgomery, William Y Tang, Ale-
jandro Cuadron, Chenguang Wang, Raluca Ada Popa, and Ion Stoica.
Judgebench: A benchmark for evaluating llm-based judges. arXiv preprint
arXiv:2410.12784 , 2024.
[49] DeepSeek Team. DeepSeek-R1-Lite-Preview. DeepSeek website , 2024.
https://chat.deepseek.com/ (last accessed 6/12/2024).
[50] Gemini Team. Gemini 1.5: Unlocking multimodal understanding across
millions of tokens of context. arXiv preprint arXiv:2403.05530 , 2024.
[51] Qwen Team. QwQ: Reflect deeply on the boundaries of the unknown,
November 2024.
[52] S. Toulmin. The Uses of Argument . Cambridge University Press, Cam-
bridge, England, 1958.
[53] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez,  Lukasz Kaiser, and Illia Polosukhin. Attention is all you
need. Advances in neural information processing systems , 30, 2017.
24[54] Bart Verheij. Evaluating arguments based on toulmin’s scheme. Argumen-
tation , 19:347–371, 2005.
[55] D Walton, C. Reed, and F. Macagno. Argumentation Schemes . Cambridge
University Press, Cambridge, UK, 2008.
[56] Douglas N Walton. Argumentation schemes for presumptive reasoning .
Psychology Press, 1996.
[57] Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra,
Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al.
Mmlu-pro: A more robust and challenging multi-task language understand-
ing benchmark. arXiv preprint arXiv:2406.01574 , 2024.
[58] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia,
Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elic-
its reasoning in large language models. Advances in Neural Information
Processing Systems , 35:24824–24837, 2022.
[59] Colin White, Samuel Dooley, Manley Roberts, Arka Pal, Ben Feuer, Sid-
dhartha Jain, Ravid Shwartz-Ziv, Neel Jain, Khalid Saifullah, Siddartha
Naidu, et al. Livebench: A challenging, contamination-free llm benchmark.
arXiv preprint arXiv:2406.19314 , 2024.
[60] Zhiheng Xi, Dingwen Yang, Jixuan Huang, Jiafu Tang, Guanyu Li, Yiwen
Ding, Wei He, Boyang Hong, Shihan Do, Wenyu Zhan, et al. Enhancing llm
reasoning via critique models with test-time and training-time supervision.
arXiv preprint arXiv:2411.16579 , 2024.
[61] Shiyang Yu and Frank Zenker. Schemes, critical questions, and complete
argument evaluation. Argumentation , 34(4):469–498, 2020.
[62] Yifan Zhang, Jingqin Yang, Yang Yuan, and Andrew Chi-Chih Yao.
Cumulative reasoning with large language models. arXiv preprint
arXiv:2308.04371 , 2023.
[63] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao
Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al.
Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in
Neural Information Processing Systems , 36:46595–46623, 2023.
25Appendix
In this section, we detail all the prompts devised and used within our CQoT
pipeline and the experiment conducted to evaluate its performance.
CQoT Pipeline Prompt Templates
As depicted in Figure 2, the CQoT approach follows a procedural structure
organised into 4 different steps. Each step is driven by an operation executed
by the underlying LLM which, in turn, is triggered by a specific prompt. Every
prompt includes multiple tags enclosing diverse sets of instructions, as shown
in Figure 7. Intuitively, the main command is contained within the ‘System
Instruction’ labels and makes use of the content defined by the ‘User Prompt’
and‘Reasoning Steps’ tags. Notice that Step 3 of the pipeline acts as the
loop counter for the overarching process, backtracking to the initial stage if
the produced reasoning plan does not positively address the critical questions
posited in Step 2. Given that this operation is either automated or manually
executed, there is no need for any LLM to intervene, hence, no prompt.
<System Instruction> You are a critical 
and analytical reasoner. Carefully read 
the user prompt. Instead of replying, 
provide your step -by-step reasoning, 
clearly dividing and outlining the 
different steps. Each step must be very 
specific, easy to follow and the set of 
premises should logically lead to a 
truthful conclusion. Do NOT provide the 
final answer (but don't write down 'do not 
provide the final answer’ in the reasoning 
process). </End System Instruction>  
 
<User Prompt> {input}</End User Prompt>  
 
Remember not to provide the final answer, 
just spell out the reasoning plan.  
 Prompt for STEP 1  
   . The replies should be ONLY 'Yes' or 'No’  
    sdg sdgd 
    sgasdg 
    sgsdg 
    afdgadfg  
    sdga 
    sdgasg 
    sdgsd 
    dsgsdg 
    dsgfsdg 
    sdgsdg 
    sdg Prompt for STEP 2  
<System Instruction> You are an analytical and critical reasoner. Assess the provided reasoning steps 
against the following questions using the user prompt as a context. Reply simply with 'Yes' or 'No' for 
each question. Be very strict and strongly critical and answer 'No' when in doubt. Do NOT answer or 
reply to anything else than this.  
1.Does the reasoning process start with clearly defined premises?  
2.Are the premises supported by evidence or accepted facts?  
3.Does the reasoning process use logical connections between premises and conclusions?  
4.Are the logical connections used in the reasoning process valid?  
5.Does the reasoning process avoid fallacies or logical errors?  
6.Is the conclusion logically derived from the premises?  
7.Is the reasoning process consistent with established knowledge or principles?  
8.Does the reasoning process lead to a conclusion that is plausible and reasonable?                
</End System Instruction>  
 
<User Prompt> {same input as STEP 1} </End User Prompt>  
 
<Reasoning Steps> {generated from STEP 1} </End Reasoning Steps>  
  
 Remember: you MUST reply to EACH question. The replies should be ONLY 'Yes' or 'No'  
 
<System Instruction> You are a skilled, critical and analytical reasoner. You always reason in steps according to the provided instruction. Give a n answer to 
the user prompt by utterly and strictly following the protocol established by the reasoning steps. Here are the stages of wha t you have to do: 1) Carefully read 
and understand the protocol detailed by the reasoning steps. Then, start your reasoning. 2) Each intermediate conclusion of y our reasoning MUST be yielded by the 
truthfulness of its respective premises. 3) Each established conclusion MUST NOT contradict other information. After each ste p, look back at previous steps and 
check that you are not contradicting what you previously deduced. 4) Your final reply MUST be logically entailed by the reaso ning steps and each established 
conclusion. Meticulously ensure that your reply is consistent with your reasoning.  </End System Instruction>  
 
<User Prompt> {same input as STEP 1} </End User Prompt>  
 
<Reasoning Steps> {generated from STEP 1} </End Reasoning Steps>  Prompt for STEP 4  Prompt for STEP 3  
No prompt  
Figure 7: CQoT pipeline prompts for Steps 1, 2 and 4. Recall that the function of Step 3 is
only to check and, if required, reiterate the previous two stages.
26Pipeline Prompt Templates
According to Figure 4, the pattern followed in the ablation study comprises
only the first and last steps of the CQoT pipeline. This serves to test the
results achieved by the underlying LLM when its thinking process is steered
by the preliminary generation of a reasoning plan without any assessment from
the critical questions. As such, the structure presented in Figure 8 embeds
only the prompts of Step 1 and Step 4, which are composed exactly as the
ones introduced in Figure 7, hence designed with the same ‘System Instruction’ ,
‘User Prompt’ and‘Reasoning Steps’ tags.
<System Instruction> You are a critical 
and analytical reasoner. Carefully read 
the user prompt. Instead of replying, 
provide your step -by-step reasoning, 
clearly dividing and outlining the 
different steps. Each step must be very 
specific, easy to follow and the set of 
premises should logically lead to a 
truthful conclusion. Do NOT provide the 
final answer (but don't write down 'do not 
provide the final answer’ in the reasoning 
process). </End System Instruction>  
 
<User Prompt> {input}</End User Prompt>  
 
Remember not to provide the final answer, 
just spell out the reasoning plan.  
 Prompt for STEP 1  
<System Instruction> You are a skilled, critical and analytical reasoner. You always reason in steps according to the provided instruction. Give a n answer to 
the user prompt by utterly and strictly following the protocol established by the reasoning steps. Here are the stages of wha t you have to do: 1) Carefully read 
and understand the protocol detailed by the reasoning steps. Then, start your reasoning. 2) Each intermediate conclusion of y our reasoning MUST be yielded by 
the truthfulness of its respective premises. 3) Each established conclusion MUST NOT contradict other information. After each  step, look back at previous steps 
and check that you are not contradicting what you previously deduced. 4) Your final reply MUST be logically entailed by the r easoning steps and each established 
conclusion. Meticulously ensure that your reply is consistent with your reasoning.  </End System Instruction>  
 
<User Prompt> {same input as STEP 1} </End User Prompt>  
 
<Reasoning Steps> {generated from STEP 1} </End Reasoning Steps>  Prompt for STEP 4  
Figure 8: Pipeline prompts for ablation study. Steps 1 and 4 present the same input as the
respective stages of Figure 7.
CoT Prompt Template
Chain-of-Thought is a well-known prompting technique capable of enhancing
LLMs output by adding, in its most trivial form, the words “Let’s think step
by step” . In our CoT prompt, illustrated in Figure 9, we adopted a similar
phrasing within the standard structure specification ( ‘System Instruction’ ,‘User
Prompt’ ) provided in each prompt of the CQoT pipeline.
LLM Judge Prompt Templates
The assessment of each model output (either baseline, CoT augmented, Step 1
and 4-only, or CQoT) has been conducted by GPT-4o (October 2024 version),
which assumed the role of the judge. Both MT-Bench Reasoning and Math tasks
27<System Instruction> You are a critical 
and analytical reasoner. Reason step by 
step and provide your final reply to the 
following user prompt. </End System 
Instruction>  
 
<User Prompt> {input}</End User Prompt>  
 
 Prompt for CoT Figure 9: Input used to augment baseline models with Chain-of-Thought reasoning.
consist of 40 questions, each of which is split into two sub-questions: an initial
query and a sequent one strictly interconnected with the first. For every sub-
question, the judge needs to use a different prompt, as underscored in Figure 10.
Besides the examined LLM’s replies (enclosed within the ‘Assistant’s Answer’
tags), such prompts also present a reference answer (defined by the ‘Reference
Answer’ tags) whose purpose is to facilitate the evaluation by rendering the
correct response of the specific question available to the judge. However, despite
the given input, it may occur that GPT-4o would still be influenced by the
length of the assessed model’s replies. When this happened, we prompted the
judge with a follow-up input, reminding it that conciseness should not affect the
evaluation score, especially if the answer is correct (Figure 11).
<System Instruction> Please act as an impartial judge and 
evaluate the quality of the response provided by an AI 
assistant to the user question displayed below. Your evaluation 
should consider correctness and helpfulness. You will be given 
a reference answer and the assistant’s answer. Begin your 
evaluation by comparing the assistant’s answers with the 
reference answer. Identify and correct any mistakes. Do not 
allow the length of the responses to influence your evaluation. 
Be as objective as possible. After providing your explanation, 
please rate the response on a scale of 1 to 10 by strictly 
following this format: “[[rating]]”, for example: “Rating: 
[[5]]” </End System Instruction>  
 
<User Prompt> {question} </End User Prompt>  
 
<Start of Reference Answer> {ref_answer }</End of Reference 
Answer> 
 
<Start of Assistant’s Answer> {answer} </End of Assistant’s            
 Answer> 
 
 Reference -guided  single answer  grading  
 <System Instruction> Please act as an impartial judge and 
evaluate the quality of the response provided by an AI assistant 
to the user question displayed below. Your evaluation should 
consider correctness and helpfulness. You will be given a 
reference answer and the assistant’s answer. Begin your 
evaluation by comparing the assistant’s answers with the 
reference answer. Identify and correct any mistakes. Do not allow 
the length of the responses to influence your evaluation. Be as 
objective as possible. After providing your explanation, please 
rate the response on a scale of 1 to 10 by strictly following 
this format: “[[rating]]”, for example: “Rating: [[5]]” </End 
System Instruction>  
 
<User Prompt> {question1},{question2} </End User Prompt>  
 
<Start of Reference Answer> {ref_answer1},{ref_answer2} </End 
of Reference Answer>  
 
<Start of Assistant’s Answer> {answer} </End of Assistant’s            
 Answer> 
 
 Reference -guided  single answer  grading  (two questions ) 
 
Figure 10: Prompts leveraged by the LLM judge to score the other models’ responses.
28 
 
Utterly and completely disregard the length 
of the reply in your evaluation. The 
reference answer only underscores the 
correct reply, not its specific length. The 
assistant's answer can be longer, more 
detailed and further explained than the 
reference's answer without being deducted 
points during its assessment. If correct, 
this additional information could also 
ensure a higher evaluation score.   Prompt  for disregarding  conciseness  Figure 11: Input adjusting judge evaluation in case it unnecessarily penalises the length of
the response.
29