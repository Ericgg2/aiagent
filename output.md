The paper "Can gender categorization influence the perception of animated virtual humans?" explores the impact of gender categorization on the perception of animated virtual babies. Conducted by researchers at the Pontifical Catholic University of Rio Grande do Sul, Brazil, the study builds on previous work that indicates emotional responses may vary based on the perceived gender of a character, even when the character itself is genderless. 

The researchers created a 3D animated virtual baby model and presented three stimuli, manipulating only the name assigned to the baby to represent male, female, or unnamed. They aimed to replicate and expand upon an earlier study, which showed that participants' emotional reactions could differ significantly based on the gendered name of the baby. 

Key hypotheses were developed:
1. Men and women perceive emotions similarly across male, female, and unnamed virtual babies (H01).
2. The unnamed baby is recognized as genderless (H02).
3. Perceived comfort and charisma differ for babies with gendered names compared to the unnamed baby (H03).

The study involved 168 participants and used a questionnaire to measure emotional responses, perceived comfort, and charisma. They found that:
- Women reported higher emotional responses to the baby with a female name, while men showed increased emotional responses for the male-named and unnamed babies. This refutes H01.
- The majority of participants attributed a gender to the unnamed baby, signaling a persistent gender bias (supporting H02).
- Women rated the female-named baby as more comfortable and charismatic, while men showed a preference for the male-named baby. The unnamed baby was perceived as the least charismatic by both genders (refuting H03).

The results indicate that subtle cues, such as names, can significantly influence perceptions of virtual characters' emotions, comfort, and charisma, emphasizing the existence of gender biases in virtual environments. The paper concludes that the animation industry might not need to create heavily gendered characters, as perception can be effectively influenced by other simpler attributes like names. Future research is suggested to explore a wider range of virtual human characteristics to understand gender attribution further and potentially inform the animation of genderless characters.

---

## Paper Summary: http://arxiv.org/pdf/2402.03328v2

Date: 

The paper titled "Visual Enumeration is Challenging for Large-scale Generative AI" by Alberto Testolin, Kuinan Hou, and Marco Zorzi explores the capabilities of large-scale generative artificial intelligence (AI) systems in understanding and processing visual numerosity — the ability to perceive the number of objects in a visual scene without counting. 

The authors begin by highlighting the innate ability of humans and many animals, including infants, to make numerical judgments about small sets of objects, which becomes less precise for larger collections. They define this intuitive grasp of number, referred to as visual number sense, and outline its significance for subsequent mathematical learning. 

The study investigates whether large generative AI systems can mimic this ability by reliably identifying the number of items in visual stimuli or generating images with specific counts of objects (in the range of 1-10). Several foundation models, including GPT-4V, DALL-E, and Stable Diffusion, are tested for their numerosity naming (identifying the number of objects in an image) and numerosity production (creating images with a target number of items) abilities.

Key findings reveal that the majority of foundation models exhibit poor performance in both tasks. For instance, models like ViLT and BLIP-2 consistently struggle to accurately name the number of objects, with confusion matrices showing little systematic variation in their responses. They tend to produce stereotyped answers that reflect common layouts from their training data while failing to abstract numerical information from object categories. Their overall accuracy is alarmingly low, with most scoring below 30%.

In contrast, the more advanced models, GPT-4V and Gemini, demonstrate slightly better performance with around 70% accuracy in naming tasks, suggesting some rudimentary enumeration skills. Their responses show consistency across categories and are often correct in the subitizing range (1-4 items). The study characterizes these models as "Six"-knowers, indicating a more developed numerical understanding than earlier models.

For the numerosity production task, results indicate that generative models produced images with low accuracies, although DALL-E 2 occasionally performed better within certain category boundaries. Stable Diffusion was classified as a "Two"-knower, DALL-E 2 as a "Three"-knower, and DALL-E 3 as a "Four"-knower based on their performance against established criteria for numerical understanding found in children.

Further analysis revealed that response distributions from models often did not reflect a scaling relationship consistent with Weber's law, which governs human numerical cognition. Scalar variability was only observed in GPT-4V and DALL-E 3, indicating these models might grasp some aspects of numerosity more effectively than their predecessors.

The authors conclude that current large-scale generative AI systems still struggle significantly with visual enumeration, reflecting a level of understanding comparable to young children who do not fully command counting principles. The potential for emergent number sense in models remains largely uncharted, raising the need for more open-source foundations that prioritize developing perceptual capabilities like visual numerosity.

The findings underline the challenges faced by AI in achieving human-like numeric understanding and suggest that significant advancements in AI architecture and training are necessary to bridge the gap between artificial and human cognitive abilities in this domain.

---

## Paper Summary: http://arxiv.org/pdf/1809.01581v1

Date: 

The paper discusses the development of a dialogue management system designed to facilitate multiparty interactions between artificial agents (a physical robot and an animated virtual human) and infants aged 6 to 12 months, with the goal of helping infants learn visual sign language, specifically American Sign Language (ASL). This age is critical for language development, and the authors aim to create engaging and socially contingent interactions.

The system, named RAVE (Robot, Avatar, thermal Enhanced language learning tool), focuses on maintaining the infant's engagement and eliciting socially contingent responses. It uses multimodal perception tools, such as an eye-tracker to measure attention and a thermal infrared camera to assess emotional arousal, aiming to determine when infants are "ready to learn" based on their emotional engagement.

The study tested RAVE with eight infants, reporting that all showed spontaneous and sustained engagement, actively participating in the conversation through relevant social behaviors. A case-study analysis demonstrated the infants' significant interaction with the avatars, showing that even without prior exposure to sign language, infants attempted to mimic the linguistic cues and signs presented by the animated avatar.

Key findings include:
1. The ability of the system to capture infants' attention and maintain their engagement during interactions, lasting significantly longer than typical attention spans for this age group.
2. The infants produced socially contingent responses to agent communications, indicating the potential for the agents to act as linguistic facilitators.
3. Parent involvement in the interaction allowed for additional social referencing, enhancing infants' learning opportunities.

The authors conclude that the RAVE system holds the potential to support language learning in young infants, particularly those with minimal language exposure during crucial developmental periods, and suggest further development for autonomous operations and adaptability to individual learning patterns. Future research aims to refine the system for broader applications and potential use in real-world environments, ultimately benefiting infants' language acquisition and interaction experiences.

---

## Paper Summary: http://arxiv.org/pdf/2208.02386v1

Date: 

The paper investigates the influence of gender categorization on the perception of animated virtual humans, particularly focusing on how participants’ emotional responses vary depending on the assigned gender of a virtual baby. The authors replicate a previous study that identified gender bias in the perception of real infants and demonstrate similar effects with virtual babies. Participants were divided into three groups based on whether the baby was identified as male, female, or unnamed. 

The study revealed that emotional responses varied by virtual baby gender: women felt more emotion towards the female-named baby, whereas men felt more emotion towards both the male-named and unnamed babies. This suggests that even without visible cues indicating gender, the name alone can shape emotional perceptions. The authors affirm that gender categorization leads to significant biases in emotional assessment, supporting their hypothesis.

Furthermore, the study examined the comfort and charisma associated with the virtual babies, revealing that participants identified as male felt more comfortable with the male-named baby, while female participants found the female-named baby more charismatic. This indicates a clear gender bias in the perceived comfort and appeal of virtual human characters.

The findings extend to suggest that gendered attributes in virtual humans do not need to rely solely on stereotypical visual indicators, as emotional perception can be influenced through simple design choices like naming. This has important implications for developers in the field of computer graphics, highlighting the benefits of potentially moving away from traditional gender stereotypes in the modeling of virtual humans.

In conclusion, the study emphasizes that gender perception in virtual representations is deeply influenced by societal stereotypes, even in controlled environments, thereby affecting how different genders are perceived. The results hold significance for both psychological understanding of bias and practical applications in virtual human design.

---



---

