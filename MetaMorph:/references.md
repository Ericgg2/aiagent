Armen Aghajanyan, Bernie Huang, Candace Ross, Vladimir Karpukhin, Hu Xu, Naman Goyal, Dmytro Okhonko,
Mandar Joshi, Gargi Ghosh, Mike Lewis, et al. Cm3: A causal masked multimodal model of the internet. arXiv
preprint arXiv:2201.07520 , 2022.
Pravesh Agrawal, Szymon Antoniak, Emma Bou Hanna, Devendra Chaplot, Jessica Chudnovsky, Saurabh Garg,
Theophile Gervet, Soham Ghosh, Amélie Héliou, Paul Jacob, et al. Pixtral 12b. arXiv preprint arXiv:2410.07073 ,
2024.
AI@Meta. Llama 3 model card. 2024.
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch,
Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. In NeurIPS ,
2022.
Zeyuan Allen-Zhu. ICML 2024 Tutorial: Physics of Language Models, 2024. Project page: https://physics.allen-zhu.
com/.
Anthropic. Claude, 2024.
Jimmy Lei Ba, Jamie Kiros, and Geoffrey E. Hinton. Layer normalization. In NeurIPS , 2016.
Adrien Bardes, Quentin Garrido, Jean Ponce, Xinlei Chen, Michael Rabbat, Yann LeCun, Mahmoud Assran, and
Nicolas Ballas. Revisiting feature prediction for learning visual representations from video. In TMLR, 2024.
Florian Bordes, Randall Balestriero, and Pascal Vincent. High fidelity visualization of what your self-supervised
representation knows about. In TMLR, 2022.
Tim Brooks, Aleksander Holynski, and Alexei A Efros. Instructpix2pix: Learning to follow image editing instructions.
InCVPR, 2023.
Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v:
Improving large multi-modal models with better captions. arXiv preprint arXiv:2311.12793 , 2023.
Lin Chen, Xilin Wei, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Bin Lin, Zhenyu
Tang, et al. Sharegpt4video: Improving video understanding and generation with better captions. In NeurIPS ,
2024a.
Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo,
Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source
suites. arXiv preprint arXiv:2404.16821 , 2024b.
11Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale N
Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning. In
NeurIPS , 2024.
Runpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng Ge, Jinrong Yang, Liang Zhao, Jianjian Sun, Hongyu
Zhou, Haoran Wei, et al. Dreamllm: Synergistic multimodal comprehension and creation. In ICLR, 2024.
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa
Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers
for image recognition at scale. In ICLR, 2021.
Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten,
Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. Datacomp: In search of the next generation of multimodal
datasets. In NeurIPS , 2024.
Peng Gao, Renrui Zhang, Chris Liu, Longtian Qiu, Siyuan Huang, Weifeng Lin, Shitian Zhao, Shijie Geng, Ziyi Lin,
Peng Jin, et al. Sphinx-x: Scaling data and parameters for a family of multi-modal large language models. arXiv
preprint arXiv:2402.05935 , 2024.
Yuying Ge, Yixiao Ge, Ziyun Zeng, Xintao Wang, and Ying Shan. Planting a seed of vision in large language model.
arXiv preprint arXiv:2307.08041 , 2023.
Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne Westphal, Heuna Kim,
Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz Mueller-Freitag, et al. The" something something" video
database for learning and evaluating visual common sense. In ICCV, 2017a.
Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the v in vqa matter: Elevating
the role of image understanding in visual question answering. In CVPR, 2017b.
Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415 , 2016.
Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: A reference-free evaluation
metric for image captioning. In EMNLP, 2021.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a
two time-scale update rule converge to a local nash equilibrium. In NeurIPS , 2017.
Minyoung Huh, Brian Cheung, Tongzhou Wang, and Phillip Isola. The platonic representation hypothesis. In ICML,
2024.
Oğuzhan Fatih Kar, Alessio Tonioni, Petra Poklukar, Achin Kulshrestha, Amir Zamir, and Federico Tombari. Brave:
Broadening the visual encoding of vision-language models. In ECCV, 2025.
Jing Yu Koh, Daniel Fried, and Russ R Salakhutdinov. Generating images with multimodal language models. In
NeurIPS , 2024.
Benno Krojer, Dheeraj Vattikonda, Luis Lara, Varun Jampani, Eva Portelance, Christopher Pal, and Siva Reddy.
Learning action and reasoning-centric image editing from videos and simulations. In NeurIPS , 2024.
Hugo Laurençon, Lucile Saulnier, Léo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang,
Siddharth Karamcheti, Alexander Rush, Douwe Kiela, et al. Obelics: An open web-scale filtered dataset of interleaved
image-text documents. Advances in Neural Information Processing Systems , 36, 2024a.
Hugo Laurençon, Léo Tronchon, Matthieu Cord, and Victor Sanh. What matters when building vision-language
models? arXiv preprint arXiv:2405.02246 , 2024b.
Yann LeCun. A path towards autonomous machine intelligence version 0.9. 2, 2022-06-27. Open Review , 62(1):1–62,
2022.
Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and
Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326 , 2024a.
Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al.
Mvbench: A comprehensive multi-modal video understanding benchmark. In CVPR, 2024b.
Tianhong Li, Dina Katabi, and Kaiming He. Return of unconditional generation: A self-supervised representation
generation method. In NeurIPS , 2024c.
12Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence
Zitnick. Microsoft coco: Common objects in context. In ECCV, 2014.
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS , 2023.
Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In CVPR,
2024a.
Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved
reasoning, ocr, and world knowledge, 2024b.
Hao Liu, Wilson Yan, Matei Zaharia, and Pieter Abbeel. World model on million-length video and language with
ringattention. arXiv preprint arXiv:2402.08268 , 2024c.
Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui
He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? In ECCV, 2024d.
I Loshchilov. Decoupled weight decay regularization. In ICLR, 2019.
Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, and Aniruddha Kembhavi. Unified-io: A unified
model for vision, language, and multi-modal tasks. In ICLR, 2022a.
Jiasen Lu, Christopher Clark, Sangho Lee, Zichen Zhang, Savya Khosla, Ryan Marten, Derek Hoiem, and Aniruddha
Kembhavi. Unified-io 2: Scaling autoregressive multimodal models with vision language audio and action. In CVPR,
2024.
Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark,
and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. In
NeurIPS , 2022b.
Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: A benchmark for question
answering about charts with visual and logical reasoning. In ACL, 2022.
Brandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier, Sam Dodge, Bowen Zhang, Philipp Dufter, Dhruti Shah,
Xianzhi Du, Futang Peng, Floris Weers, et al. Mm1: Methods, analysis & insights from multimodal llm pre-training.
arXiv preprint arXiv:2403.09611 , 2024.
Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and Josef Sivic. Howto100m:
Learning a text-video embedding by watching hundred million narrated video clips. In ICCV, 2019.
OpenAI. gpt4o, 2024.
Jiayi Pan, Yichi Zhang, Nicholas Tomlin, Yifei Zhou, Sergey Levine, and Alane Suhr. Autonomous evaluation and
refinement of digital agents. In COLM, 2024a.
Xichen Pan, Li Dong, Shaohan Huang, Zhiliang Peng, Wenhu Chen, and Furu Wei. Kosmos-g: Generating images in
context with multimodal large language models. In ICLR, 2024b.
Konpat Preechakul, Nattanat Chatthee, Suttisak Wizadwongsa, and Supasorn Suwajanakorn. Diffusion autoencoders:
Toward a meaningful and decodable representation. In CVPR, 2022.
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda
Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision.
InICML, 2021.
Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward training
trillion parameter models. In SC20: International Conference for High Performance Computing, Networking, Storage
and Analysis , pages 1–16. IEEE, 2020.
Adam Roberts, Colin Raffel, Katherine Lee, Michael Matena, Noam Shazeer, Peter J Liu, Sharan Narang, Wei Li, and
Yanqi Zhou. Exploring the limits of transfer learning with a unified text-to-text transformer. JMLR, 2019.
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image
synthesis with latent diffusion models. In CVPR, 2022.
Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo
Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for
training next generation image-text models. In NeurIPS , 2022.
13Hao Shao, Shengju Qian, Han Xiao, Guanglu Song, Zhuofan Zong, Letian Wang, Yu Liu, and Hongsheng Li. Visual
cot: Advancing multi-modal language models with a comprehensive dataset and benchmark for chain-of-thought
reasoning. In NeurIPS , 2024.
Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and Amanpreet Singh. Textcaps: a dataset for image captioning
with reading comprehension, 2020.
Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun
Huang, and Xinlong Wang. Generative multimodal models are in-context learners. In CVPR, 2024a.
Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao, Jingjing Liu, Tiejun
Huang, and Xinlong Wang. Generative pretraining in multimodality. In ICLR, 2024b.
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B.
Hashimoto. Alpaca: A strong, replicable instruction-following model, 2023.
Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818 , 2024.
Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang,
Shusheng Yang, Adithya Iyer, Xichen Pan, et al. Cambrian-1: A fully open, vision-centric exploration of multimodal
llms. In NeurIPS , 2024a.
Shengbang Tong, Erik Jones, and Jacob Steinhardt. Mass-producing failures of multimodal systems with language
models. In NeurIPS , 2024b.
Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. Eyes wide shut? exploring the
visual shortcomings of multimodal llms. In CVPR, 2024c.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,
Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. LLaMA 2: Open foundation and fine-tuned chat models.
2023.
Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin
Ge, et al. Qwen2-vl: Enhancing vision-language model’s perception of the world at any resolution. arXiv preprint
arXiv:2409.12191 , 2024a.
Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang,
Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869 , 2024b.
Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and
Quoc V Le. Finetuned language models are zero-shot learners. In ICLR, 2022a.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al.
Chain-of-thought prompting elicits reasoning in large language models. In NeurIPS , 2022b.
Chengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu,
Chong Ruan, et al. Janus: Decoupling visual encoding for unified multimodal understanding and generation. arXiv
preprint arXiv:2410.13848 , 2024a.
Penghao Wu and Saining Xie. V*: Guided visual search as a core mechanism in multimodal llms. In CVPR, 2024.
Yecheng Wu, Zhuoyang Zhang, Junyu Chen, Haotian Tang, Dacheng Li, Yunhao Fang, Ligeng Zhu, Enze Xie, Hongxu
Yin, Li Yi, et al. Vila-u: a unified foundation model integrating visual understanding and generation. arXiv preprint
arXiv:2409.04429 , 2024b.
xAI. grok, 2024.
Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie
Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding
and generation. arXiv preprint arXiv:2408.12528 , 2024.
Hu Xu, Saining Xie, Xiaoqing Ellen Tan, Po-Yao Huang, Russell Howes, Vasu Sharma, Shang-Wen Li, Gargi Ghosh,
Luke Zettlemoyer, and Christoph Feichtenhofer. Demystifying clip data. In ICLR, 2024.
Tian Ye, Zicheng Xu, Yuanzhi Li, and Zeyuan Allen-Zhu. Physics of Language Models: Part 2.1, Grade-School
Math and the Hidden Reasoning Process. ArXiv e-prints , abs/2407.20311, 2024. Full version available at http:
//arxiv.org/abs/2407.20311 .
14Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming
Ren, Yuxuan Sun, et al. Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark
for expert agi. In CVPR, 2024a.
Xiang Yue, Tianyu Zheng, Yuansheng Ni, Yubo Wang, Kai Zhang, Shengbang Tong, Yuxuan Sun, Ming Yin, Botao
Yu, Ge Zhang, et al. Mmmu-pro: A more robust multi-discipline multimodal understanding benchmark. arXiv
preprint arXiv:2409.02813 , 2024b.
Mert Yuksekgonul, Federico Bianchi, Pratyusha Kalluri, Dan Jurafsky, and James Zou. When and why vision-language
models behave like bags-of-words, and what to do about it? In ICLR, 2022.
Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training.
InICCV, 2023.
Yuexiang Zhai, Hao Bai, Zipeng Lin, Jiayi Pan, Shengbang Tong, Yifei Zhou, Alane Suhr, Saining Xie, Yann LeCun,
Yi Ma, et al. Fine-tuning large vision-language models as decision-making agents via reinforcement learning. In
NeurIPS , 2024.
Ruohong Zhang, Liangke Gui, Zhiqing Sun, Yihao Feng, Keyang Xu, Yuanhan Zhang, Di Fu, Chunyuan Li, Alexander
Hauptmann, Yonatan Bisk, et al. Direct preference optimization of video large multimodal models from language
model reward. arXiv preprint arXiv:2404.01258 , 2024.
Yuhui Zhang, Brandon McKinzie, Zhe Gan, Vaishaal Shankar, and Alexander Toshev. Pre-trained language models do
not help auto-regressive text-to-image generation. In EMNLP, 2023.
Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili
Yu, et al. Lima: Less is more for alignment. In NeurIPS , 2024a.
Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma,
Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal
model. arXiv preprint arXiv:2408.11039 , 2024b.
Orr Zohar, Xiaohan Wang, Yonatan Bitton, Idan Szpektor, and Serena Yeung-levy. Video-star: Self-training enables
video instruction tuning with any supervision. In arXiv preprint arXiv:2407.06189 , 2024.
15Appendix
A Training Details and Hyperparameters
A.1 MetaMorph Training
We follow the training recipe outlined in prior studies (Tong et al., 2024a; McKinzie et al., 2024), using a
two-stage training approach. First, we pretrain a two-layer MLP with a GELU activation (Hendrycks and
Gimpel, 2016) as the adapter between the visual tokens and the LLM. We train this adapter on Cambrian
adapter data while excluding all data points sourced from LAION (Schuhmann et al., 2022). Next, we finetune
the entire model, excluding the vision backbone, using the instruction tuning data described in Section 2.2
and detailed in Appendix C.
We use DeepSpeed (Rajbhandari et al., 2020) Zero-3 to train our model on H100 GPUs. Detailed training
hyperparameters for all experiments are provided in Table 2. We conduct all of the experiments with 1 epoch.
Backbone Data Adapter Instruction Tuning
Experiment LLM Adapter Instruction Tuning lr wd bs lr wd bs
Section 3 (LLaMA-3 8B) LLaMA-3 8B Cambrian Adapter Data∗Section 3 Experiment Setting 4.90e-5 0.0 768 6.93e-5 0 1536
Section 3 (LLaMA-3.1 8B) LLaMA-3.1 8B Cambrian Adapter Data∗Section 3 Experiment Setting 4.90e-5 0.0 768 6.93e-5 0 1536
Section 3 (LLaMA-3 70B) LLaMA-3 70B Cambrian Adapter Data∗Section 3 Experiment Setting 4.90e-5 0.0 768 4.90e-5 0 768
MetaMorph LLaMA-3.1 8B Cambrian Adapter Data∗All Data from Section 2.2 4.90e-5 0.0 768 6.93e-5 0 1536
Table 2 Implementation details and hyperparameters for all experiments.∗We exclude data points in LAION (Schuhmann
et al., 2022) from Cambrian adapter data.
A.2 Diffusion Visualizer Training
We leverage pretrained diffusion models such as Stable Diffusion 1.5 (Rombach et al., 2022). We use a 2-layer
MLP projector to align the SigLIP embedding dimension with the cross-attention dimension in the pretrained
diffusion model. The first layer applies a linear transformation to map the input dimension to 2048, followed
by layer normalization (Ba et al., 2016) and a ReLU activation. The second layer reduces the 2048-dimensional
features to the output dimension through a linear transformation, followed by a final layernorm.
We set the batch size to 2112. The learning rate schedule begins with a logarithmic warm-up over the first
2000 steps, gradually increasing from zero to a peak value of 1.1e-5. After this warm-up phase, the learning
rate decreases linearly over the next 12000 steps until reaching zero. We use the AdamW (Loshchilov, 2019)
optimizer to train our model, with βparameters (0.9,0.999). We apply a weight decay of 0.01.
During diffusion training, we freeze the VAE encoder and Siglip encoder, only training the projector and
the diffusion U-Net. The CFG level is set to 0.7. This is because we start with a pretrained diffusion model
and aim to transform the conditioning from CLIP text to SigLIP image embeddings. A higher CFG level
ensures the model maintains high image quality while gradually adapting to the new conditioning in the
remaining fraction. Empirically, this approach achieves the best balance between adaptation and image quality.
For the training datasets, since we finetune the diffusion model to condition on SigLIP image embeddings,
training this model does not require text descriptions for conditioning. Instead, we use images curated through
in MetaCLIP (Xu et al., 2024) and train this diffusion model to visualize the visual tokens generated by
MetaMorph.
A.3 Evaluation Benchmarks
For evaluation, we use nine ImageQA, one VideoQA and two generation benchmarks:
•MMBench (Liu et al., 2024d): A comprehensive benchmark spans across 20 multimodal ability dimensions.
•Seed(Ge et al., 2023): A benchmark focusing on visual tasks for multimodal understanding, consists of
19k multiple choice questions with accurate human annotations.
•V*STAR(Wu and Xie, 2024): A VQA benchmark designed for testing details in high-resolution images.
16Loss Image QA
AVG
MMBenchEN
SEED
RealworldQA
MMVP
SQA
MMMU
VStar
ChartQA
TextVQA
None (VQA Only) 55.50 73.11 69.96 55.69 41.33 80.39 37.29 46.60 35.16 59.96
L1 Loss 53.83 72.17 69.28 57.25 34.67 79.00 34.00 45.55 32.40 60.17
Cosine Sim 55.93 73.78 71.36 55.03 44.00 79.83 35.29 47.64 36.60 59.79.
Table 3 Comparison of different loss functions. Training with cosine similarity loss enables the model to effectively utilize
non-VQA data, which in turn enhances its visual understanding.
•MMVP(Tong et al., 2024c): A benchmark for evaluating “CLIP-Blind” pairs in Vision Language Models.
•MMMU(Yue et al., 2024a): A benchmark designed to evaluate multimodal models on extensive multi-
discipline tasks requiring college-level subject knowledge and deliberate reasoning.
•ChartQA (Masry et al., 2022): A large-scale benchmark involving visual and logical reasoning over charts.
•TextVQA (Sidorov et al., 2020):A benchmark designed to evaluate models’ ability to read and reason
about text in images to answer questions.
•ScienceQA (Lu et al., 2022b): A multimodal benchmark for answering science-related questions requiring
integration of visual and textual data.
•RealWorldQA (xAI, 2024): A benchmark focused on real-world multimodal reasoning tasks.
•MV-Bench (Li et al., 2024b): A benchmark contains a comprehensive video understanding benchmark,
which covers 20 challenging video tasks that cannot be effectively solved with a single frame.
•FID Score (Heusel et al., 2017): A metric for evaluating the quality of generated images by comparing
their feature distributions with real images.
•CLIP Score (Hessel et al., 2021): A benchmark metric that uses CLIP embeddings to measure alignment
between generated images and their corresponding text descriptions.
B Ablation Studies on Visual Prediction Objective
We compare our approach to the commonly used L1 regression loss, which has been widely adopted in
contrastive self-supervised learning methods (LeCun, 2022; Bardes et al., 2024). For this comparison, we
train MetaMorph, based on LLaMA-3 8B, using datasets described in Section 2.2. We highlight that cosine
similarity and L1 loss influence the embedding outputs differently: cosine similarity enforces normalization,
while L1 loss does not. This discrepancy in output normalization prevents a direct and fair comparison in
terms of generation performance. Consequently, our analysis focuses exclusively on VQA performance.
In Table 3, we compare models trained using L1 loss and cosine similarity loss. Our analysis reveals that
training with cosine similarity results in better average performance and outperforms L1 loss on most
benchmarks. Notably, these vision loss functions affect only tasks requiring visual predictions and do not
directly influence VQA tasks, as the VQA training data does not include image token responses. This
improvement is potentially because training with cosine similarity enhances visual generation, which in turn
contributes to better visual understanding.
To further investigate, we compare our method—incorporating a broader range of non-VQA data alongside
Cambrian-7M—–with a baseline trained exclusively on Cambrian-7M. The results show that combining
broader dataset with cosine similarity loss leads to better performance across multiple benchmarks. This
finding reinforces our earlier observations in Section 3: enhancing visual generation capabilities contributes to
improved visual understanding, highlighting the benefits of leveraging non-VQA data.
17I
m
a
g
e
Q
A
G
e
n
e
r
a
t
i
o
n
V
i
d
e
o
Q
A
P
u
r
e
V
i
d
e
o
ImageQA (44.0%) Generation (31.1%)
Cambrian-7M (Tong et al., 2024a) (7067.0 K) MetaCLIP (Xu et al., 2024) (5000.0 K)
VideoQA (9.9%) Visual Thinking (3.2%)
VideoStar (Zohar et al., 2024) (1055.0 K) VisualCoT (Shao et al., 2024) (361.0 K)
ShareVideo (Zhang et al., 2024) (540.0 K) VStar (Wu and Xie, 2024) (148.0 K)
Pure Video (8.8%) Image-to-Image (3.0%)
HowTo100M (Miech et al., 2019) (1193.0 K) InstructPix2Pix (Brooks et al., 2023) (313.0 K)
SmthSmthV2 (Goyal et al., 2017a) (220.0 K) Aurora (Krojer et al., 2024) (169.0 K)
Figure 11 Data composition. Left: The inner circle shows the distribution of MetaMorph data. Right:All the data
sources and categories in the MetaMorph data.
C Data
C.1 Data Composition
We summarize the categorization of data and the number of samples for each source in Figure 11. This
diverse dataset is curated to showcase that an LLM can be finetuned across a variety of tasks, where each
task contributes to and enhances the performance of others, as discussed in Section 3.1.
C.2 Data Proprocessing
As discussed in Section 2.2, we use a wide range of data, spanning from visual question answering tasks to
unlabeled video data. Here, we detail the preprocessing steps applied to each data source to convert them
into instruction-tuning-style QA conversations.
ImageQA. We use Cambrian-7M (Tong et al., 2024a), a dataset already curated in instruction tuning format.
An example entry looks like the below:
Example from ImageQA
Prompt:
<image_start><image><image_end> What is the animal in the image?
Response:
It is a burmilla cat.
VideoQA. We use VideoStar (Zohar et al., 2024) and ShareVideo (Chen et al., 2024a), both curated in an
instruction tuning format. For each video, we extract frames at a rate of one frame per second and input
these frames into the LLM. An example QA entry for an 8-second video is structured as follows:
Example from VideoQA
Prompt:
<image_start><image><image_end><image_start><image><image_end><image_-
start><image><image_end><image_start><image><image_end><image_start><image><image_-
end><image_start><image><image_end><image_start><image><image_end><image_-
start><image><image_end>
What’s the color of the dog in this video? (a) white (b) yellow (c) black Please only answer a single letter
and nothing else
Response:
b
Generation data. We use image-text pairs in MetaCLIP (Xu et al., 2024). The original data consists of
18images paired with corresponding text descriptions. We add system prompts and define answering formats,
transforming the image-text pairs into question-answer formats suitable for instruction tuning.
Example from Generation data
Prompt:
Generate an image of a puppy.
Response:
Here is an image based on your request: <image_start><image><image_end> .
Unlike in ImageQA and VideoQA, we require the model to predict the visual tokens in the response.
Visual Thinking Data. We explore incorporating vision as part of the model’s reasoning process to enhance its
answers. As a preliminary step, we experiment with the Visualization-of-Thought (Shao et al., 2024) and
VStar (Wu and Xie, 2024) datasets. Originally, these datasets were designed to teach models how to utilize
external tools and APIs, such as segmentation or zoom-in cropping. In this work, we aim to integrate these
visual skills directly into the model’s inference steps. We use system prompts, such as “think visually before
you answer the question” , to activate this visual reasoning mode. Here is an example:
Example from Visual thinking data
Prompt:
Think visually before you answer the question: what is the color of the hat in the image <image_-
start><image><image_end> .
Response:
Let’s consider this visually: <image_start><image><image_end> black.
In this example, the model first predicts a zoomed-in view of the key region in the image necessary for
answering the question, and then it provides the answer.
Image to image data. The model is required to predict a transformed image based on a given text description.
We use Aurora (Krojer et al., 2024) and InstructPix2Pix (Brooks et al., 2023) datasets to train this capability.
An example is presented below:
Example from Image to image data
Prompt:
<image_start><image><image_end> Make this a rainy day.
Response:
<image_start><image><image_end> .
Here, the model is expected to predict the visual tokens of the transformed image as per the specified prompt.
Pure video data. We explore commonly used open-source video datasets in instruction tuning: Something-
SomethingV2 (Goyal et al., 2017a) and HowTo100M (Miech et al., 2019). We design the following tasks from
the pure video:
1) Forward Frame Prediction. In this task, the model is presented with the initial frame of a video sequence
and must predict the subsequent frames at fixed time intervals. An example is presented below:
19Example of Forward Frame Prediction
Prompt:
<image_start><image><image_end> Can you predict what happens in the next 3 frames, each 5 seconds
apart?
Response:
<image_start><image><image_end><image_start><image><image_end><image_-
start><image><image_end>
2) Partial Sequence Completion. This task requires the model to complete a video sequence when given only
a subset of frames while maintaining temporal coherence:
Example of Partial Sequence Completion
Prompt:
<image_start><image><image_end><image_start><image><image_end><image_-
start><image><image_end> Can you predict the 2 missing frames in this 5-second-interval sequence?
Response:
<image_start><image><image_end><image_start><image><image_end>
3) Reverse Temporal Prediction. This task challenges the model to reconstruct the preceding frames given the
final frame of a sequence:
Example of Reverse Temporal Reasoning
Prompt:
<image_start><image><image_end> Work backwards to predict the previous 4 frames, each 5 seconds
apart.
Response:
<image_start><image><image_end><image_start><image><image_end><image_-
start><image><image_end><image_start><image><image_end>
4) Temporal Sequence Reordering. In this task, the model receives a shuffled sequence of video frames and
must reconstruct their correct temporal order:
Example of Temporal Sequence Reordering
Prompt:
<image_start><image><image_end><image_start><image><image_end><image_-
start><image><image_end><image_start><image><image_end>
Arrange these frames in their correct temporal sequence.
Response:
<image_start><image><image_end><image_start><image><image_end><image_-
start><image><image_end><image_start><image><image_end>
Each task is designed to train the model’s temporal understanding and visual reasoning capabilities.
C.3 Potential Image Leakage in Testing Data
When selecting data sources, we carefully choose those that do not overlap with the testing sets of our
evaluation data, such as COCO (Lin et al., 2014). However, given that the data used in a Section 2.2 is
composed of numerous sources, some degree of data leakage may be inevitable. As discussed and analyzed in a
prior work (Tong et al., 2024a), even when image overlap occurs, it does not necessarily imply that the exact
image-question pairs have been encountered during training. Unlike traditional unimodal computer vision
research, where an image alone constitutes a data point, the multimodal paradigm treats each image-text
(question-answer) pair as a distinct and unique data point.
20Joint train With Other Data # of Generation Data FID Score
Yes 1k 68.5
No 1k 115.0
Yes 5k 19.2
No 5k 116,4
Yes 10k 18.7
No 10k 111.0
Yes 50k 17.1
No 50k 111.8
Yes 200k 15.2
No 200k 110.7
Yes 200k 14.7
No 200k 93.7
Yes 1M 14.4
No 1M 52.8
Yes 3M 15.1
No 3M 39.2
Yes 5M 14.3
No 5M 27.7.
Table 4 Results of training solely on generation data vs. joint training with additional data. These results correspond to
Figure 2. Joint training with additional data significantly improves generation performance. At 5,000 samples, the
model begins to generate reasonably accurate visual tokens, indicating that visual generation is an ability unlocked
through the learning of other tasks.
Joint training Data Data Type FID Score CLIP Score
None - 110.5 5.7
Image-to-Image Other Visual Data 97.5 6.4
Visual Thinking Other Visual Data 93.5 6.5
Pure Video Other Visual Data 84.7 8.1
VideoQA Visual Understanding Data 26.5 16.1
ImageQA Visual Understanding Data 18.9 22.0.
Table 5 Impact of joint training 200k generation data with different data types. These results correspond to Figure 3. Among
the data types analyzed, joint training with visual understanding data has the most significant impact on enhancing
visual generation performance.
D Generating Visual Tokens
Here, we include the quantitative results of all the experiments in Section 3.
D.1 Results of Samples Needed to Unlock Visual Generation
Table 4 presents the quantitative results corresponding to Figure 2, which examines generation performance
under two conditions: training exclusively on generation data and joint training with all other data described
in Section 2.2. The results demonstrate that the model can develop the ability for visual generation with a
relatively modest amount of data when trained jointly with understanding tasks. In contrast, teaching this
skill in isolation requires a substantially larger dataset.
In Table 5, we present the quantitative results corresponding to Figure 3, which investigates the impact of
joint training on generation data in combination with various types of data outlined in Section 2.2. The results
show that joint training with visual understanding data—–specifically ImageQA and VideoQA–—provides the
most significant improvement in visual generation performance.
D.2 Results of Joint training Different Understanding and Generation Data
In Table 6, we present the numerical results of joint training with varying scales of understanding data (1M,
4M, 7M) and generation data (200k, 500k, 1M, 2M, 3M, 4M). These findings demonstrate that increasing the
21Data Composition Image QA Generation
# of VQA Data # of Generation Data
Average
MMBenchEN
SEED
RealworldQA
MMVP
SQA
MMMU
VStar
ChartQA
TextVQA
FID Score
CLIP Score
1M 200k 46.4 60.0 62.2 50.3 24.0 80.0 38.4 37.4 16.4 48.8 28.3 15.2
1M 500k 48.2 66.4 63.2 50.8 24.3 80.4 39.9 38.7 18.2 51.6 28.1 15.9
1M 1M 49.1 70.1 65.2 52.2 21.3 80.0 39.5 38.7 20.4 54.6 27.3 16.5
1M 2M 49.9 67.8 66.0 50.2 30.3 80.2 38.9 39.0 21.8 54.8 23.1 17.8
1M 3M 51.1 71.3 67.1 55.4 33.0 79.5 38.8 37.4 22.7 55.0 21.1 21.1
1M 4M 51.4 71.1 66.9 52.4 31.0 80.5 39.8 41.1 24.0 56.0 18.4 22.3
4M 200k 53.8 73.1 68.8 55.0 34.7 81.2 38.5 44.0 29.5 59.2 21.4 20.5
4M 500k 53.3 73.0 69.9 55.3 32.7 80.6 40.2 39.3 29.6 58.9 16.0 24.8
4M 1M 54.2 73.8 69.6 54.9 33.3 82.1 36.6 45.6 32.4 59.9 16.0 24.8
4M 2M 53.8 72.8 70.3 55.2 37.3 80.8 36.8 44.0 31.2 56.2 15.6 24.7
4M 3M 54.3 71.8 70.1 57.7 36.0 81.0 38.0 42.9 32.6 59.0 16.1 24.8
4M 4M 54.4 75.2 69.9 56.0 37.3 81.4 38.1 40.8 31.6 59.3 15.3 25.5
7M 200k 55.8 73.1 70.3 55.6 42.0 81.0 40.8 44.0 35.2 60.6 18.2 22.3
7M 500k 55.6 74.4 70.6 56.2 38.7 81.9 37.9 44.0 36.0 60.5 15.2 25.5
7M 1M 55.8 74.3 70.3 56.3 42.7 81.3 36.6 44.5 35.8 60.6 14.5 26.6
7M 2M 55.4 73.9 71.1 56.9 40.0 81.6 35.9 42.4 35.4 61.6 14.8 27.1
7M 3M 55.6 74.2 71.0 57.3 38.0 81.1 40.1 43.5 35.0 60.2 14.2 27.5
7M 4M 56.2 75.4 70.4 55.4 44.0 80.4 39.6 45.0 35.2 60.2 14.9 26.3.
Table 6 Full results of joint training on varying amounts of VQA data (1M, 4M, 7M) and generation data (200k, 500k, 1M, 2M, 3M,
4M).These results correspond to Figure 4, Figure 5, Figure 7, and Figure 8, which analyze how different combinations
of understanding and generation data impact the model’s visual understanding and generation performance.
Pretrained LLM Image QA Generation
LLM
Average
MMBenchEN
SEED
RealworldQA
MMVP
SQA
MMMU
VStar
ChartQA
TextVQA
FID Score
CLIP Score
LLaMA-3 8B 55.8 74.3 70.3 56.3 42.7 81.3 36.6 44.5 35.8 60.6 14.5 26.6
LLaMA-3.1 8B 56.7 75.8 70.2 56.2 44.7 81.9 41.2 43.4 36.0 61.3 13.2 27.1
LLaMA-3 70B 60.7 80.7 72.6 58.3 48.7 87.8 48.9 47.1 37.4 65.0 13.8 26.8.
Table 7 Full results of training on different LLMs. We train 7M VQA data and 1M generation data on different LLM
backbones (LLaMA-3 8B, LLaMA-3.1 8B, and LLaMA-3 70B) and measure understanding and generation performance.
amount of understanding data yields more substantial improvements in both understanding tasks (e.g., VQA
performance) and generation tasks (e.g., FID scores and CLIP scores) compared to increasing the amount of
generation data. These results, consistent with our analysis in Section 3.2 and Section 3.3, highlight that
understanding data play a more pivotal role in enhancing performance across both task types.
D.3 Results of Training on Different LLMs
We present the results of training with 7M VQA data and 1M generation data across various LLM backbones,
including LLaMA-3 8B, LLaMA-3.1 8B, and LLaMA-3 70B. As shown in Table 7, which corresponds to
the results in Figure 6, we observe that stronger LLM backbones lead to improvements in both visual
understanding and visual generation. These findings further support the conclusion that visual understanding
and generation are reciprocal processes, where advancements in one drives enhancements in the other.
E More Examples of MetaMorph
We provide additional examples of MetaMorph in Figure 12 and Figure 13. These examples illustrate how
MetaMorph extends beyond the capabilities of typical MLLMs by leveraging learned skills to perform novel
tasks such as visual reasoning and visual transformation. In Figure 12, when prompted with the question
22“What is the type of hat?” , MetaMorph first generates visual tokens related to hats and then answers correctly
with“top hat” . The model also demonstrates the ability to perform image transformations, such as creating
a cartoon version of an image or altering it to appear as daytime. Additionally, we showcase examples of
MetaMorph solving implicit puzzles, such as interpreting “a rearrangement of the letters in the word ’tca”’ ,
before generating the corresponding visual tokens of cats.
23Make a child cartoon from it
<image_start ><image_end >
Generate an image of the pet animal whose 
name is a rearrangement of the letters in the 
word ' tca’
Here's the generated image based on 
your request: < image_start ><image_end >
What's the animal in this image?
The animal in the image is a domestic 
cat, specifically a tabby cat. Tabbies are 
characterized by their distinctive coat 
patterns, which include stripes, dots, and 
swirling patterns. The markings on this 
cat's face, with the 'M' shape on its 
forehead and the stripes on its ears, are 
typical of a tabby pattern.
Reflect on the visual details: What's the type of 
hat that animal wear in the image?
Let's imagine the critical detail: 
<image_start ><image_end > The animal is 
wearing a top hat.
How many birds are in the image and why?
There is only one bird in the image. The 
bird is captured in a single frame, and its 
reflection in the water creates the illusion 
of two birds.
Is the coke in the ads regular or zerosugar ?
The coke in the ads is zero sugar.Figure 12 Examples of MetaMorph (I). We showcase examples of MetaMorph’s capabilities: transforming images based
on prompts ( top-left), answering challenging questions ( top-right ), integrating visual tokens into reasoning processes
(bottom-left ), implicitly solving puzzles ( bottom-right ) and answering tricky video-qa questions. ( bottom).
24Generate an image of a puppy
Here's the generated image based on 
your request: < image_start ><image_end >
What’s this city?
The city in the image is New York City, 
specifically the Lower Manhattan area, 
which is home to the iconic World Trade 
Center complex. The presence of the 
One World Trade Center, which is the 
tallest building in the image, is a 
distinctive feature.
Can you turn it to view during the daytime?
<image_start ><image_end >Generate an image of Kagu
Here's the generated image based on 
your request: < image_start ><image_end >
Figure 13 Examples of MetaMorph (II). We showcase more examples of MetaMorph’s capabilities: answering questions and
transforming images in one conversation ( left), generating images ( top-right ), and leveraging knowledge in LLMs to
generate rare concepts ( bottom-right ).
25