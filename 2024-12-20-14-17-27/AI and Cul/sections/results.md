Our
study
evaluates
the
foundational
knowledge
of
eight
LLMs
using
the
CNSWE
across
three
test
conditions.
Our
first
test
condition
is
the
primary
test,
as
it
is
consistent
with
the
actual
testing
procedures.
We
then
compare
the
scores
on
this
first
test
condition
with
two
variations
to
gain
further
insight
into
model
performance.
Condition
1:
Requir ed
Response
Format
In
this
condition,
models
were
instructed
to
answer
every
question.
Seven
of
the
eight
models
scored
above
the
60-point
threshold
on
the
jurisprudence
and
applied
knowledge
tests,
with
only
Deepseek
falling
slightly
below
the
threshold
(59.5)
on
the
applied
knowledge
section.
The
models
exceeded
chance
performance
by
a
wide
margin.
Across
all
models,
performance
was
generally
stronger
on
the
jurisprudence
than
on
the
applied
knowledge
test
(median
scores
=
71.0
and
66.0,
respectively).
The
jurisprudence
test
showed
wider
performance
variation,
with
scores
ranging
from
62.0
to
84.0,
compared
to
a
more
restricted
range
on
the
applied
knowledge
test
(59.5
to
71.0).
When
comparing
Chinese
and
Western
models,
Chinese
models
scored
slightly
higher
on
the
jurisprudence
test
(median
=
77.0
vs.
70.3)
but
somewhat
lower
on
the
applied
knowledge
test
(median
=
65.5
vs.
67.0).
Figure
1
provides
a
breakdown
of
the
scores
for
each
model
by
test
type.
Six
of
eight
LLMs
performed
better
on
the
jurisprudence
than
on
the
applied
knowledge
test.
On
the
jurisprudence
test,
the
Chinese
models,
Ernie
and
Qwen,
led
by
a
substantial
margin
(84.0
and
83.0),
exceeding
their
nearest
competitors
by
ten
percentage
points.
Claude-Sonnet
(Western)
tied
for
third
with
a
score
of
73.0,
while
Mistral
(Western)
had
the
lowest
score
of
62.0.
Performance
patterns
differed
on
the
applied
knowledge
test,
where
Mistral
achieved
the
highest
score
(71.0),
followed
closely
by
Qwen
(69.0)
and
Gemini
(68.0).
While
Mistral
showed
manifest
improvement
from
its
jurisprudence
score,
gaining
nine
percentage
points,
Ernie
demonstrated
the
most
dramatic
shift
between
tests,
dropping
18
points
from
its
jurisprudence
score
of
84.0
to
66.0
on
the
applied
knowledge
test.15
Figure
1.
Performance
Comparison
of
Chinese
and
Western
Large
Language
Models
on
the
Jurisprudence
and
Applied 
Knowledge
Test
of
the
Chinese
National
Social
Work
Examination.
Combined
Composite
Scores
are
the
sum
of
the
Applied
Knowledge
Test
(100
points)
and
the
Jurisprudence
Test
(100
points; 
Total
possible
points
=
200).
Models
tested:
Ernie-4.0-8k,
Qwen-Max,
DeepSeek-2.5,
Moonshot-v1-8k,
Claude-3.5-Sonnet, 
Gemini-1.5-Pro,
ChatGPT-4o,
and
Mistral-Large.
As
demonstrated
in
Figure
2,
performance
patterns
on
the
jurisprudence
test
revealed
notable
differences
between
single-select
and
SATA
questions.
The
left
panel
shows
raw
scores,
while
the
right
panel
displays
normalized
scores,
dividing
raw
scores
by
the
total
points
possible
for
each
question
type
(60
points
for
single-select
and
40
points
for
SATA).
In
both
raw
and
normalized
analyses,
Chinese
models
demonstrated
superior
performance
on
single-select
questions,
with
Ernie
and
Qwen
achieving
normalized
scores
above
85.0%,
significantly
outperforming
their
Western
counterparts.
However,
these
same
models
showed
marked
declines
in
performance
on
normalized
SATA
scores,
dropping
to
the
mid-70%
range.
In
contrast,
three
Western
models
(Claude,
ChatGPT,
and
Gemini)
showed
slightly
improved
performance
on
normalized
SATA
scores
compared
to
their
normalized
single-select
scores.
However,
their
overall
scores
remained
lower
than
the
Chinese
models.
Mistral
maintained
relatively
consistent
normalized
performance
across
both
question
types
(62.0%
for
single-select
and
61.0%
for
SATA).
Across
all
models,
normalized
performance
was
generally
stronger
on
single-select
questions
(median
score
=
74.5%)
compared
to
SATA
questions
(median
score
=
65.0%),
suggesting
that
the
SATA
format
presented
a
particular
challenge
for
the
highest-performing
Chinese
models
in
assessing
jurisprudence
knowledge.
16
Figure
2.
Performance
Comparison
Between
Single-Select
and
Select-All-That-Apply
(SATA)
Questions
on
the 
Jurisprudence
Test
of
the
Chinese
National
Social
Work
Examination.
Models
tested:
Ernie-4.0-8k,
Qwen-Max,
DeepSeek-2.5,
Moonshot-v1-8k,
Claude-3.5-Sonnet,
Gemini-1.5-Pro,
ChatGPT-4o, 
and
Mistral-Large.
SATA
=
Select-all-that-apply.
Looking
at
Figure
3,
the
applied
knowledge
test
performance
patterns
reveal
a
substantial
contrast
between
single-select
and
SATA
questions
across
all
models.
The
raw
score
analysis
shows
models
generally
achieving
higher
performance
on
single-select
questions
than
SATA,
with
most
models
scoring
between
40-45
points
(out
of
60
possible)
on
single-select
questions
but
only
20-25
points
(out
of
40
possible)
on
SATA
questions.
This
performance
gap
becomes
even
more
pronounced
when
examining
normalized
scores
to
account
for
the
different
point
totals.
In
the
normalized
analysis,
most
models
achieved
scores
between
65-75%
on
single-select
questions
but
dropped
to
the
50-60%
range
on
SATA
questions,
representing
a
consistent
15-20
percentage
point
decline
in
performance.
This
pattern
held
across
Chinese
and
Western
models,
suggesting
that
the
SATA
format
presented
a
universal
challenge
in
assessing
applied
knowledge,
regardless
of
the
model's
origin.
This
finding
contrasts
notably
with
the
jurisprudence
test
results,
where
some
Western
models
showed
improved
performance
on
SATA
questions
compared
to
single-select
questions.
17
Figure
3.
Performance
Comparison
Between
Single-Select
and
Select-All-That-Apply
Questions
on
the
Applied
Knowledge 
Test
of
the
Chinese
National
Social
Work
Examination.
Models
tested:
Ernie-4.0-8k,
Qwen-Max,
DeepSeek-2.5,
Moonshot-v1-8k,
Claude-3.5-Sonnet,
Gemini-1.5-Pro,
ChatGPT-4o, 
and
Mistral-Large.
SATA
=
Select-all-that-apply.
Condition
2:
Option
to
Skip
Format
As
shown
in
Table
2,
when
given
the
option
to
skip
questions
they
were
uncertain
about
on
the
jurisprudence
test,
five
models
exercised
this
option,
while
three
(Deepseek,
Claude,
and
Gemini)
attempted
all
questions.
ChatGPT
and
Mistral
demonstrated
the
most
conservative
approach,
each
skipping
four
questions.
Moonshot
skipped
two
questions,
while
Qwen
and
Ernie
each
skipped
one
question
despite
being
the
top
performers.
The
option
to
skip
questions
produced
minor
improvements
in
adjusted
scores
(i.e.,
percent
correct),
while
performance
patterns
largely
mirrored
those
seen
in
Condition
1.
ChatGPT's
accuracy
improved
from
61.2%
to
64.5%
when
accounting
for
skipped
questions,
and
Mistral's
score
increased
from
55.0%
to
57.9%.
Qwen
moved
up
one
position
to
claim
the
top
spot,
while
Ernie
dropped
one
position
to
second
place.
The
bottom
three
models
(ChatGPT,
Moonshot,
and
Mistral)
maintained
their
relative
positions
from
Condition
1,
suggesting
that
skipping
questions
did
not
significantly
impact
their
comparative
standing
on
the
jurisprudence
section.
In
contrast
to
the
jurisprudence
test,
models
showed
different
patterns
of
question-skipping
behavior
on
the
applied
knowledge
test
(see
Table
3).
Only
three
models
skipped
questions.
Specifically ,
ChatGPT
and
Mistral
each
skipped
one
question,
while
Qwen
skipped
one
question
despite
its
high
18
performance.
The
option
to
skip
questions
produced
minor
improvements
in
adjusted
scores
(i.e.,
percent
correct),
while
performance
patterns
showed
notable
differences
from
Condition
1.
Ernie
achieved
the
highest
score
with
71.2%,
showing
no
skipped
questions,
while
Qwen's
score
improved
slightly
from
68.8%
to
69.6%
when
accounting
for
its
skipped
question.
Ernie
moved
up
three
positions
to
claim
the
top
spot,
and
Moonshot
showed
substantial
improvement
by
moving
up
four
positions
to
tie
for
third
place
with
Claude
and
Gemini,
each
scoring
66.2%.
The
most
dramatic
shift
occurred
with
Mistral,
which
dropped
six
positions
despite
skipping
a
question,
while
ChatGPT
dropped
three
positions,
ending
with
adjusted
scores
of
64.6%.
Table
2.
Performance
Comparison
of
Chinese
and
Western
Large
Language
Models
on
the
Jurisprudence
Sections
of
the 
Chinese
Social
Work
Professional
Level
Examination
With
an
Option
to
Skip.
Condition2RankModelTotalquestionsskippedTotalpercentcorrect-w/SkipTotalpercentcorrectw/oSkipChangeinRank(comparedtoCondition1)
1Qwen181.0%80.0%+1
2Ernie178.5%77.5%-1
3Deepseek070.0%70.0%+1
4Claude066.2%66.2%-1
5Gemini065.0%65.0%-1
6ChatGPT464.5%61.2%0
7Moonshot257.7%56.2%0
8Mistral457.9%55.0%0
Models
are
ranked
based
on
total
percent
correct
w/o
skip
in
condition
2.
Changes
indicate
the
difference
between
Condition 
2
and
Condition
1
scores.
Rankings
are
based
on
Condition
2
performance.
Models
tested:
Ernie-4.0-8k,
Qwen-Max, 
DeepSeek-2.5,
Moonshot-v1-8k,
Claude-3.5-Sonnet,
Gemini-1.5-Pro,
ChatGPT-4o,
and
Mistral-Large.19
Table
3.
Performance
Comparison
of
Chinese
and
Western
Large
Language
Models
on
the
Applied
Knowledge
Sections
of 
the
Chinese
Social
Work
Professional
Level
Examination
Condition2RankModelTotalquestionsskippedTotalpercentcorrect-w/SkipTotalpercentcorrectw/oSkipChangeinRank(comparedtoCondition1)
1Ernie071.2%71.2%+3
2Qwen169.6%68.8%0
=3Moonshot066.2%66.2%+4
=3Claude066.2%66.2%+1
=3Gemini066.2%66.2%0
6Deepseek065.0%65.0%+2
=7Mistral164.6%63.8%-6
=7ChatGPT164.6%63.8%-3
With
an
Option
to
Skip.
Models
are
ranked
based
on
total
percent
correct
w/o
skip
in
condition
2.
Changes
indicate
the 
difference
between
Condition
2
and
Condition
1
scores.
Rankings
are
based
on
Condition
2
performance.
Models
tested: 
Ernie-4.0-8k,
Qwen-Max,
DeepSeek-2.5,
Moonshot-v1-8k,
Claude-3.5-Sonnet,
Gemini-1.5-Pro,
ChatGPT-4o,
and 
Mistral-Large.
Condition
3:
Testing
for
Study
Design
Effects
Condition
3
evaluated
potential
study
design
effects
by
presenting
models
with
answer
choices
without
their
corresponding
question
stems.
This